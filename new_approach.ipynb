{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arya232004/Sign-Language-Motion-Recognition-and-Translation-for-Videos/blob/main/new_approach.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWxKYYZwCjex"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import csv\n",
        "\n",
        "# Set the path to your videos folder\n",
        "videos_folder = '/content/drive/MyDrive/ISL/New Approach /dataset'\n",
        "\n",
        "# Create lists to store the training and testing data\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "# Iterate through subfolders in the videos folder\n",
        "for subfolder in os.listdir(videos_folder):\n",
        "    subfolder_path = os.path.join(videos_folder, subfolder)\n",
        "\n",
        "    # Check if it's a directory\n",
        "    if os.path.isdir(subfolder_path):\n",
        "        videos = os.listdir(subfolder_path)\n",
        "\n",
        "        # Shuffle the list of videos\n",
        "        random.shuffle(videos)\n",
        "\n",
        "        # Calculate the split index for 80% train and 20% test\n",
        "        split_index = int(0.8 * len(videos))\n",
        "\n",
        "        # Split the videos into train and test\n",
        "        train_videos = videos[:split_index]\n",
        "        test_videos = videos[split_index:]\n",
        "\n",
        "        # Add entries to the train_data list\n",
        "        train_data.extend([(subfolder, video) for video in train_videos])\n",
        "\n",
        "        # Add entries to the test_data list\n",
        "        test_data.extend([(subfolder, video) for video in test_videos])\n",
        "\n",
        "# Define the paths for the train and test CSV files\n",
        "train_csv_path = '/content/drive/MyDrive/ISL/train.csv'\n",
        "test_csv_path = '/content/drive/MyDrive/ISL/test.csv'\n",
        "\n",
        "# Write data to CSV files\n",
        "def write_to_csv(csv_path, data):\n",
        "    with open(csv_path, 'w', newline='') as csvfile:\n",
        "        fieldnames = ['tag', 'video_name']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for entry in data:\n",
        "            writer.writerow({'tag': entry[0], 'video_name': entry[1]})\n",
        "\n",
        "# Write train data to train CSV file\n",
        "write_to_csv(train_csv_path, train_data)\n",
        "\n",
        "# Write test data to test CSV file\n",
        "write_to_csv(test_csv_path, test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SyLwejPF_csY"
      },
      "outputs": [],
      "source": [
        "from tensorflow_docs.vis import embed\n",
        "from tensorflow import keras\n",
        "from imutils import paths\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import imageio\n",
        "import cv2\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-e9kVDHJ_fbX"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
        "  except RuntimeError as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "cIeL9sXy_iGM",
        "outputId": "ff24c5cf-ce58-4845-c2b3-7d44e899c0ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total videos for training: 712\n",
            "Total videos for testing: 187\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"tag\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"eat\",\n          \"address\",\n          \"grow\"\n        ],\n        \"num_unique_values\": 8,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"video_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"samples\": [\n          \"apple_40.mp4\",\n          \"eat_17.mp4\",\n          \"christmas_97.mp4\"\n        ],\n        \"num_unique_values\": 10,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-05fc5245-678c-4f86-acd4-333ba15f2654\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>video_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>grow</td>\n",
              "      <td>grow_17.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>eat</td>\n",
              "      <td>eat_17.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>grow</td>\n",
              "      <td>grow_10.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>clock</td>\n",
              "      <td>clock_6.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>bad</td>\n",
              "      <td>bad_29.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>christmas</td>\n",
              "      <td>christmas_97.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>address</td>\n",
              "      <td>address_16.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>christmas</td>\n",
              "      <td>christmas_3.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>apple</td>\n",
              "      <td>apple_40.mp4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>dog</td>\n",
              "      <td>dog_40.mp4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05fc5245-678c-4f86-acd4-333ba15f2654')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-05fc5245-678c-4f86-acd4-333ba15f2654 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-05fc5245-678c-4f86-acd4-333ba15f2654');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d3f680a5-9859-4eb3-b748-3e2e9f0278f0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d3f680a5-9859-4eb3-b748-3e2e9f0278f0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d3f680a5-9859-4eb3-b748-3e2e9f0278f0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "           tag        video_name\n",
              "193       grow       grow_17.mp4\n",
              "162        eat        eat_17.mp4\n",
              "180       grow       grow_10.mp4\n",
              "409      clock       clock_6.mp4\n",
              "478        bad        bad_29.mp4\n",
              "79   christmas  christmas_97.mp4\n",
              "578    address    address_16.mp4\n",
              "54   christmas   christmas_3.mp4\n",
              "670      apple      apple_40.mp4\n",
              "425        dog        dog_40.mp4"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/ISL/train.csv\")\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/ISL/test.csv\")\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iu19Eg6k_1J8"
      },
      "outputs": [],
      "source": [
        "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
        "IMG_SIZE = 224\n",
        "\n",
        "\n",
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "    cap = cv2.VideoCapture(path)\n",
        "    frames = []\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV-jglgAAygI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_feature_extractor():\n",
        "    # Load InceptionV3 without top layers\n",
        "    base_model = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "\n",
        "    # Freeze the weights of the pre-trained layers\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add a global average pooling layer\n",
        "    global_average_layer = layers.GlobalAveragePooling2D()(base_model.output)\n",
        "\n",
        "    # Add a dense layer with more units (features)\n",
        "    dense_layer = layers.Dense(512, activation='relu')(global_average_layer)\n",
        "\n",
        "    # Build the model\n",
        "    model = keras.Model(inputs=base_model.input, outputs=dense_layer, name=\"feature_extractor\")\n",
        "\n",
        "    return model\n",
        "\n",
        "feature_extractor = build_feature_extractor()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPMzydFeETQ_",
        "outputId": "08de49b4-c729-4464-9fa0-a7b2e469e83f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of features: 2048\n"
          ]
        }
      ],
      "source": [
        "# Build the feature extractor model\n",
        "feature_extractor = build_feature_extractor()\n",
        "\n",
        "# Get the output shape of the feature extractor's final layer\n",
        "num_features = feature_extractor.output_shape[-1]\n",
        "\n",
        "print(\"Number of features:\", num_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z58ml9fTBBRs",
        "outputId": "aac16d84-a5e9-499a-d7bb-e8b88ef128bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['I dont know', 'address', 'apple', 'bad', 'bathroom', 'beautiful', 'bowl', 'christmas', 'clock', 'die', 'dog', 'door', 'eat', 'expensive', 'family', 'grow']\n"
          ]
        }
      ],
      "source": [
        "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
        "print(label_processor.get_vocabulary())\n",
        "\n",
        "labels = train_df[\"tag\"].values\n",
        "labels = label_processor(labels[..., None]).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7FnrHqqBItF"
      },
      "outputs": [],
      "source": [
        "#Define hyperparameters\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "\n",
        "MAX_SEQ_LENGTH = 200\n",
        "NUM_FEATURES = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK27H3Y6CUZE",
        "outputId": "08f35f62-e9d5-4ccf-cc92-26ea36bc27c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame features in train set: (712, 200, 512)\n",
            "Frame masks in train set: (712, 200)\n",
            "train_labels in train set: (712, 1)\n",
            "test_labels in train set: (187, 1)\n"
          ]
        }
      ],
      "source": [
        "def prepare_all_videos(df, root_dir):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"video_name\"].values.tolist()\n",
        "\n",
        "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
        "    labels = df[\"tag\"].values\n",
        "\n",
        "    #convert classlabels to label encoding\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
        "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
        "\n",
        "    # For each video.\n",
        "    for idx, path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.\n",
        "        frames = load_video(os.path.join(root_dir, path))\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_features = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            video_length = batch.shape[0]\n",
        "            length = min(MAX_SEQ_LENGTH, video_length)\n",
        "            for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "\n",
        "        frame_features[idx,] = temp_frame_features.squeeze()\n",
        "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
        "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
        "\n",
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"train_labels in train set: {train_labels.shape}\")\n",
        "\n",
        "print(f\"test_labels in train set: {test_labels.shape}\")\n",
        "\n",
        "# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "SgA1IveoCxh5",
        "outputId": "43e2b91b-26f6-4d0f-b3a5-3c56888ac711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7691 - accuracy: 0.0944\n",
            "Epoch 1: val_loss improved from inf to 2.78820, saving model to ./tmp/video_classifier\n",
            "16/16 [==============================] - 23s 949ms/step - loss: 2.7691 - accuracy: 0.0944 - val_loss: 2.7882 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/30\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7612 - accuracy: 0.1707\n",
            "Epoch 2: val_loss did not improve from 2.78820\n",
            "16/16 [==============================] - 13s 804ms/step - loss: 2.7612 - accuracy: 0.1707 - val_loss: 2.8038 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/30\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7534 - accuracy: 0.1707\n",
            "Epoch 3: val_loss did not improve from 2.78820\n",
            "16/16 [==============================] - 13s 798ms/step - loss: 2.7534 - accuracy: 0.1707 - val_loss: 2.8195 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/30\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7460 - accuracy: 0.1707\n",
            "Epoch 4: val_loss did not improve from 2.78820\n",
            "16/16 [==============================] - 13s 804ms/step - loss: 2.7460 - accuracy: 0.1707 - val_loss: 2.8350 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/30\n",
            "16/16 [==============================] - ETA: 0s - loss: 2.7388 - accuracy: 0.1707\n",
            "Epoch 5: val_loss did not improve from 2.78820\n",
            "16/16 [==============================] - 14s 906ms/step - loss: 2.7388 - accuracy: 0.1707 - val_loss: 2.8504 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/30\n",
            " 4/16 [======>.......................] - ETA: 8s - loss: 2.7332 - accuracy: 0.2188"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f03f810c1b55>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-f03f810c1b55>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mseq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sequence_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     history = seq_model.fit(\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.4)(x)\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model\n",
        "\n",
        "EPOCHS = 30\n",
        "# Utility for running experiments.\n",
        "def run_experiment():\n",
        "    filepath = \"./tmp/video_classifier\"\n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "\n",
        "    seq_model = get_sequence_model()\n",
        "    history = seq_model.fit(\n",
        "        [train_data[0], train_data[1]],\n",
        "        train_labels,\n",
        "        validation_split=0.3,\n",
        "        epochs=EPOCHS,\n",
        "        callbacks=[checkpoint],\n",
        "    )\n",
        "\n",
        "    seq_model.load_weights(filepath)\n",
        "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "\n",
        "    return history, seq_model\n",
        "\n",
        "\n",
        "_, sequence_model = run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpVj-B4Ad1J5",
        "outputId": "616b2a94-85ad-4a7c-e624-e24272ef6be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (544, 50, 64, 64, 3)\n",
            "y_train shape: (544, 12)\n",
            "X_test shape: (137, 50, 64, 64, 3)\n",
            "y_test shape: (137, 12)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Specify the path to the folder containing video subfolders\n",
        "data_path = '/content/drive/MyDrive/ISL/videos'\n",
        "\n",
        "# List all subfolders (assuming each subfolder is a word)\n",
        "word_folders = [folder for folder in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, folder))]\n",
        "\n",
        "# Initialize lists to store data and labels\n",
        "data = []\n",
        "labels = []\n",
        "max_frames = 200  # Set a fixed number of frames for each video\n",
        "\n",
        "# Loop through each word\n",
        "for word_index, word_folder in enumerate(word_folders):\n",
        "    word_path = os.path.join(data_path, word_folder)\n",
        "\n",
        "    # Loop through each video file in the word folder\n",
        "    for video_file in os.listdir(word_path):\n",
        "        video_path = os.path.join(word_path, video_file)\n",
        "\n",
        "        # Read video frames using OpenCV\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "\n",
        "        frame_count = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret or frame_count >= max_frames:\n",
        "                break\n",
        "\n",
        "            # Preprocess frame as needed\n",
        "            # (e.g., resize, normalize pixel values)\n",
        "            # For simplicity, we'll resize frames to 64x64 pixels\n",
        "            frame = cv2.resize(frame, (64, 64))\n",
        "            frame = frame / 255.0  # Normalize pixel values to the range [0, 1]\n",
        "\n",
        "            frames.append(frame)\n",
        "            frame_count += 1\n",
        "\n",
        "        # Organize frames into a 3D tensor (num_frames, height, width, channels)\n",
        "        video_data = np.array(frames)\n",
        "\n",
        "        # Append data and label\n",
        "        data.append(video_data)\n",
        "        labels.append(word_index)\n",
        "\n",
        "# Pad sequences to ensure all videos have the same number of frames\n",
        "data_padded = pad_sequences(data, maxlen=max_frames, padding='post', truncating='post', dtype='float32')\n",
        "\n",
        "# Convert data and labels to numpy arrays\n",
        "data = np.array(data_padded)\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Perform train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = len(word_folders)\n",
        "y_train = to_categorical(y_train, num_classes=num_classes)\n",
        "y_test = to_categorical(y_test, num_classes=num_classes)\n",
        "\n",
        "# Print shapes of train and test sets\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BmD2k7m2fqls",
        "outputId": "77dd4ce1-1ef7-4e77-d753-08af5f1bc9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_10 (Conv3D)          (None, 48, 62, 62, 16)    1312      \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPooli  (None, 24, 31, 31, 16)    0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_13 (Ba  (None, 24, 31, 31, 16)    64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 24, 31, 31, 16)    0         \n",
            "                                                                 \n",
            " conv3d_11 (Conv3D)          (None, 22, 29, 29, 32)    13856     \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPooli  (None, 11, 14, 14, 32)    0         \n",
            " ng3D)                                                           \n",
            "                                                                 \n",
            " batch_normalization_14 (Ba  (None, 11, 14, 14, 32)    128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 11, 14, 14, 32)    0         \n",
            "                                                                 \n",
            " time_distributed_6 (TimeDi  (None, 11, 6272)          0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 64)                1622272   \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 12)                780       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1638412 (6.25 MB)\n",
            "Trainable params: 1638316 (6.25 MB)\n",
            "Non-trainable params: 96 (384.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, LSTM, Dense, Flatten, BatchNormalization, Dropout, TimeDistributed\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(Conv3D(16, kernel_size=(3, 3, 3), activation='relu', input_shape=(50, 64, 64, 3)))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.8))\n",
        "\n",
        "model.add(Conv3D(128, kernel_size=(3, 3, 3), activation='relu'))\n",
        "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Apply TimeDistributed to prepare for LSTM\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(units=128, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Flatten to reduce spatial dimensions\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer for classification\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Replace num_classes with the actual number of classes\n",
        "\n",
        "# Compile the model with an appropriate optimizer, loss, and metrics\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKjzVSSEjH5t",
        "outputId": "0c759f26-0297-44e7-adf3-d8124a8d6201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14/14 [==============================] - 8s 252ms/step - loss: 2.9708 - accuracy: 0.1379 - val_loss: 2.3996 - val_accuracy: 0.1101\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 2s 168ms/step - loss: 2.0663 - accuracy: 0.3195 - val_loss: 2.4681 - val_accuracy: 0.1927\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 3s 194ms/step - loss: 1.4429 - accuracy: 0.5241 - val_loss: 2.9058 - val_accuracy: 0.0917\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 3s 213ms/step - loss: 1.0863 - accuracy: 0.6414 - val_loss: 3.0754 - val_accuracy: 0.1284\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 2s 171ms/step - loss: 0.7379 - accuracy: 0.7379 - val_loss: 3.7580 - val_accuracy: 0.0734\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 2s 169ms/step - loss: 0.5329 - accuracy: 0.8230 - val_loss: 4.2095 - val_accuracy: 0.0734\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 2s 170ms/step - loss: 0.4455 - accuracy: 0.8713 - val_loss: 4.7108 - val_accuracy: 0.0734\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 2s 170ms/step - loss: 0.2807 - accuracy: 0.9195 - val_loss: 4.0946 - val_accuracy: 0.0734\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 3s 210ms/step - loss: 0.2590 - accuracy: 0.9241 - val_loss: 3.7192 - val_accuracy: 0.0734\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 2s 173ms/step - loss: 0.2136 - accuracy: 0.9471 - val_loss: 4.0386 - val_accuracy: 0.0734\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e6c2b99dd20>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(X_train, y_train, epochs=10, batch_size=32,  validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGY7hYA_jjhm",
        "outputId": "4cf4f002-6d9c-4189-c8e5-f4e0762e9790"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 - 0s - loss: 4.0098 - accuracy: 0.0949 - 446ms/epoch - 89ms/step\n",
            "Test Loss: 4.0098\n",
            "Test Accuracy: 9.49%\n"
          ]
        }
      ],
      "source": [
        "# Assuming you have already trained the model and have it stored in the 'model' variable\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/ISL/eight_seven.h5')\n",
        "\n",
        "def preprocess_video_for_prediction(video_path, target_frames=150, frame_shape=(64, 64)):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, frame_shape)\n",
        "        frame = frame / 255.0\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < target_frames:\n",
        "        frames.extend([frames[-1]] * (target_frames - len(frames)))\n",
        "    elif len(frames) > target_frames:\n",
        "        frames = frames[:target_frames]\n",
        "\n",
        "    return np.expand_dims(np.array(frames), axis=0)\n",
        "\n",
        "video_path_to_predict = '/content/drive/MyDrive/ISL/Testing/soham_christmas.mp4'\n",
        "\n",
        "input_video_data = preprocess_video_for_prediction(video_path_to_predict, target_frames=150)\n",
        "\n",
        "predictions = model.predict(input_video_data)\n",
        "\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "class_index_to_word = {\n",
        "    0: 'expensive',\n",
        "    1: 'I dont know',\n",
        "    2: 'grow',\n",
        "    3: 'clock',\n",
        "    4: 'address',\n",
        "    5: 'apple',\n",
        "    6: 'dog',\n",
        "    7: 'beautiful',\n",
        "    8: 'bathroom',\n",
        "    9: 'eat',\n",
        "    10: 'christmas',\n",
        "    11: 'door',\n",
        "    12: 'family',\n",
        "    13: 'bad',\n",
        "    14: 'die',\n",
        "    15: 'bowl'\n",
        "}\n",
        "print(\"Predicted Class Index:\", predicted_class_index)\n",
        "print(\"predicted word is : \", class_index_to_word[predicted_class_index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLZtTnkZmNst",
        "outputId": "fbaa3962-fd6c-4510-e33f-151da0c3a03b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted Class Index: 4\n",
            "predicted word is :  address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ldXAYd9-BrCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/ISL/eight_eight.h5')\n",
        "\n",
        "def preprocess_video_for_prediction(video_path, target_frames=150, frame_shape=(64, 64)):\n",
        "    frames = []\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.resize(frame, frame_shape)\n",
        "        frame = frame / 255.0\n",
        "\n",
        "        frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < target_frames:\n",
        "        frames.extend([np.zeros_like(frames[0])] * (target_frames - len(frames)))\n",
        "    elif len(frames) > target_frames:\n",
        "        frames = frames[:target_frames]\n",
        "\n",
        "    return np.expand_dims(np.array(frames), axis=0)\n",
        "\n",
        "\n",
        "video_path_to_predict = '/content/drive/MyDrive/ISL/Testing/soham_grow.mp4'\n",
        "\n",
        "input_video_data = preprocess_video_for_prediction(video_path_to_predict, target_frames=150)\n",
        "\n",
        "predictions = model.predict(input_video_data)\n",
        "\n",
        "predicted_class_index = np.argmax(predictions)\n",
        "class_index_to_word = {\n",
        "    0: 'expensive',\n",
        "    1: 'I dont know',\n",
        "    2: 'grow',\n",
        "    3: 'clock',\n",
        "    4: 'address',\n",
        "    5: 'apple',\n",
        "    6: 'dog',\n",
        "    7: 'beautiful',\n",
        "    8: 'bathroom',\n",
        "    9: 'eat',\n",
        "    10: 'christmas',\n",
        "    11: 'door',\n",
        "    12: 'family',\n",
        "    13: 'bad',\n",
        "    14: 'die',\n",
        "    15: 'bowl'\n",
        "}\n",
        "print(\"Predicted Class Index:\", predicted_class_index)\n",
        "print(\"predicted word is : \", class_index_to_word[predicted_class_index])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5XSECp1sDQH",
        "outputId": "8f8399e1-5410-4ec8-e32e-6de9cb0c3112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Predicted Class Index: 14\n",
            "predicted word is :  die\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1gvRLqICqp0Ft0MAbvMMlwfRN7A_vbx-F",
      "authorship_tag": "ABX9TyPEHNMhc3+2MCaIBDUJk9vq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}